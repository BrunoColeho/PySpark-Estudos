# -*- coding: utf-8 -*-
"""Estudos Pyspark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15r6I5oVgs-dMAUAdcMVb6VUP4QuP2QY9
"""

!pip install pyspark==3.3.2

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
#importante pra simular o ambiente do trabalho
#também tem gente que usa import pyspark.sql.functions as F e aí quando chama o col usa F.col('...')

spark = (
    SparkSession.builder.appName('Curso de PySpark')
    .config('spark.sql.repl.eagerEval.enabled', True)
    .getOrCreate())

spark

df = spark.read.parquet('/content/LOGINS.parquet').select('email', 'senha', 'estado', 'cor_favorita', 'profissao')
#hedar = true se eu quiser informar que a primeira linha são os nomes das colunas



df.select(
    col('email').alias('e-mail'),
    col('senha').alias('password')
    )

#se eu quero selecionar tudo menos determinada coluna posso usar o comando drop, nele o argumento são as QUE EU NÃO QUERO
df.drop('email', 'senha')

# df.filter((col('estado') == 'MG') & (col('cor_favorita') == 'Azul'))
#dava pra fazer em sql
#df.filter('estado = "MG" AND cor_favorita = "Azul"')
# OU
#df.filter(col('cor_favorita').isin('Ciano','Azul'))

# OUU
#essa é boa para manutenção e rastreabilidade do código! Já que posso comentar somente um dos filtros
#envolver em () faz com que nao precise usar a \ para pular linha do código
(df.filter(col('estado')=='MG')
.filter(col('cor_favorita')=='Azul'
))

(

 df
 .withColumn('nome_estadoss', when(df.estado == 'AC', 'Acre')
                              .when(df.estado =='GO', 'Goiás')
                              .when(df.estado == 'MG', 'Minas Gerais')
                              .when(df.estado == 'MT', 'Mato Grosso')
                              .when(df.estado == 'RR', 'Roraima')
                              .when(df.estado == 'AP', 'Amapá')
                              .when(df.estado == 'RJ', 'Rio de Janeiro')
                              .when(df.estado == 'DF', 'Distrito Federal')
                              .when(df.estado == 'AC', 'Acre')
                              .when(df.estado == 'SE', 'Sergipe')
                              .when(df.estado == 'PA', 'Pará')
                              .when(df.estado == 'BA', 'Bahia')
                              .when(df.estado == 'MS','Mato Grosso do Sul')
                              .when(df.estado == 'AM', 'Amazonas')
                              .when(col("estado") == 'MA', 'Manaus')

             )


)

# manipulando STRINGS, aqui é para splitar, quebrar criando uma nova coluna chamada usuário. Criei uma nova coluna pegando somente o primeiro item do split, por isso o getItem(0)
# no ultimo getItem lembrar que o '.' é uma forma de caractere de escape no PySpark, logo pra eu dizer que o . é o divisor do split precisa de uma \ antes do ponto.
#linha da substring, passo que a partir da letra 1 me retorne até 3ª veja que aqui não é 0, mas 1
#na format_string, indico %s com 's' pq quero repassar uma STRING, se fosse um numero seria %d
# o instr me indica a posição do que quero achar, se retornar 0 significa que ele não encontrou essa string dentro da substring
# repeat, para repetir o valor da coluna pela quantidade de vezes que passar no argumento
# comando trim
#regexp_replace substituir uma string por outra em uma coluna de um df
#lpad-esquerda / rpad-direita passo a coluna, digo quantos caracteres quero que tenha e passo depois entre '' com qual caractere quero que preencha até atingir o argumento
(
    df
      #.withColumn('usuario', split(df.email, '@').getItem(0))
      #.withColumn('provedor', split(df.email, '@').getItem(1))
      #.withColumn('nome_provedor', split(col('provedor'), '\.').getItem(0))
      #.withColumn('concatenada', concat(col('profissao'), lit('_'), col('cor_favorita')))
      #.withColumn('minusculando', lower(col('profissao')))
      #.withColumn('maiusculando', upper(col('cor_favorita')))
      #.withColumn('PrimeiraMaiusc', initcap(col('minusculando')))
      .withColumn('substring', substring(col('cor_favorita'), 1, 3))
      #.withColumn('format_string', format_string('Olá, %s. Sua cor favorita é %s', col('email'), col('cor_favorita')))
      #.withColumn('instr', instr(col('email'), '@gmail'))
      #.withColumn('length', length(col("profissao")))
      #.withColumn('Repeat', repeat(col('estado'), 5))
      # .withColumn('coluna_espacos', concat(lit('      '), col('estado'), lit('      ')))
      # .withColumn('trim', trim(col('coluna_espacos')))
      .withColumn('rpad', rpad(col("cor_favorita"), 10, '>'))
    .show(20, False)
)

#manipulando numeros
#round vai arredondar dependendo do valor e numero de casas
# ceil arredonda para o maior/próximo numero inteiro
# floor faz o contrário do ceil
# abs ele transforma em valores absolutos, exemplo é um numero negativo e quero deixálo positivo
# pow é potenciação, no caso vc passa o argumento, 2 ao quadrado, 3 ao cubo, etc,etc.
# sqrt é a raiz quadrada
# aplicando em conjunto para calcular o imc
(

 df
#  .withColumn('round', round(col('peso'),1))
  #  .withColumn('ceil', ceil(col('peso')))
  #  .withColumn('floor', floor(col('peso')))
  #  .withColumn('abs', abs(col('peso')))
  #  .withColumn('pow', pow(col('peso'),2))
  #  .withColumn('sqrt', sqrt(col('pow')))
   .withColumn('imc', round(pow(col('peso'),2) / col('altura'),1))



)

df = spark.read.parquet('/content/IMC.parquet')

df = spark.read.parquet('/content/LOGINS.parquet')

#d - dia (dd)
#M - Mês (MM)= mês com zero a esquerda // (MMM) = dec, jan, Jul, Jun, etc // (MMMM) = december, july, january, April, etc..
#y - ano (yy ou yyyy)
#E diz o dia da semana abreviado, mon, tue, wed, fri, // EEEE - dia da semana, mas nome inteiro

#add_months faz com que desloque/ande a quantidade de meses que coloquei como argumento depois de passar a coluna
#caso queira diminuir o mês, basta passar o argumento como numero negativo -1, -12, etc
#date_add para somar dias, legal para colocar data de expiração e etc. da pra colocar um negativo pra diminuir ou usar date_sub
#importante no date_format, se usar m retorna MINUTO, se usar M retorna MÊS. Outra coisa, se quero com um digito M, dois dígitos MM. Posso fazer dd/MM/yyyy, yyyy-MM-dd, etc, etc.
#datediff para calcular a diferença entre as datas, passo os dois argumentos separados por ,. Ali dividi por 365 pq quero em anos a idade
#last_day informa em que dia aquele mês termina. qual o ultimo dia do mês
#months_between calcula a diferença entre meses. Igual o datediff, mas com meses
#next_day eu passo o argumento, ali no caso "Mon" de monday para saber qual dia será a próxima segunda-feira logo após a data de cadastro, que foi o argumento inicial
#to_date converte uma string em formato date, desde que a string esteja escrita como uma data
(
    df
    .withColumn('add_months', add_months(col("data_cadastro"), 12))
    # .withColumn('current_date', current_date())
    # .withColumn('current_timestamp', current_timestamp())
    # .withColumn('date_add', date_add(col("data_cadastro"), 15))
    # .withColumn('date_format', date_format(col('data_de_nascimento'), 'E, yyyy-MMM-dd'))
    .withColumn('datediff', datediff(current_date(), col("data_de_nascimento"))/365)
    # .withColumn('dayofmonth', dayofmonth(col("data_de_nascimento")) )
    # .withColumn('dayofweek', dayofweek(col("data_de_nascimento")) )
    # .withColumn('weekofyear', dayofmonth(col("data_de_nascimento")) )
    # .withColumn('year', year(col("data_de_nascimento")) )
    # .withColumn('month', dayofmonth(col("data_de_nascimento")) )
    # .withColumn('last_day', last_day(col("data_de_nascimento")) )
    .withColumn('months_between', months_between(current_date(), col("data_de_nascimento"))/12)
    .withColumn('next_day', next_day(col("data_cadastro"), "Mon"))
    .withColumn("make_date", make_date(lit(2020), lit(8), lit(1)))
    .withColumn('to_date', to_date(lit('2020-08-01')))








    .show(20, False)
)

df.orderBy(desc(col('email')))

#agrupamento é bem importante! Exemplo, com estado e cor favorita no count, tenho duas cores cinzas no amazonas, por exemplo
(
  df.groupBy(col('estado'),col('cor_favorita')).count()
  #.groupBy(col('estado')).sum('count')
)

df = df.withColumn('num', dayofmonth(col('data_de_nascimento')))
df

(
    df.groupBy(col('cor_favorita')).agg(
        count('*'),
        sum(col('num')),
        max(col('num')),
        min(col('num')),
        stddev(col('num')))

    )

autores = spark.read.parquet('/content/AUTORES.parquet')
livros = spark.read.parquet('/content/LIVROS.parquet')
clientes = spark.read.parquet('/content/CLIENTES.parquet')
compras = spark.read.parquet('/content/COMPRAS.parquet')

autores.join(other=livros, on='id', how='left')

compras.join(other=livros, on=compras.cd_livro == livros.id, how='inner')

# para realizar o union entre dfs é preciso que eles TENHAM a MESMA quantidade de colunas e que elas ESTEJAM na MESMA ORDEM. caso estejam em ordem diferente, ele irá empilhar dados errados. Se não tiver a mesma quantidade de colunas, irá quebrar
# caso não seja possível manipular para colocar na mesma ordem, mas eu sei que as colunas dos DFs tem OS MESMOS nomes, posso usar a função df1.unionByName(df2) que ele fará a ordenação correta
# caso eu queira forçar o union entre dfs com qtde de colunas diferentes, devo fazer df.unionByName(df2, allowMissingColumns=True). Assim ele permite coluna nula

#regexp_replace
 # passar (\D) faz com que exclua os dígitos não numéricos
 #existe a função .isin, mas não existe a função isNotIn, então vc pode passar o '~'na frente da expressão e então usar o isin, assim vc vai negar a expressão.
 #exempl: .where(~col('estado).isin('SP', 'RJ')) // assim estamos pegando todos os estados que NÃO ESTÃO com SP e RJ

(
    df
    .withColumn('tel', regexp_replace(col('telefone'), ' ', '').cast('int'))
    .orderBy(desc(col('tel')))
    .orderBy(asc_nulls_last(col('tel')))
    .where(year(col('data_de_nascimento')).between(2010, 2015))
)

#PIVOT seria pegar os valores que estão em linhas e transformálos em colunas
#UNPIVOT fazer o inverso - valores 'colunados" transformar em linhas

(
    compras
    .withColumn('mes', date_format(col('data'), 'MMMM'))
    .groupBy(col("cartao_bandeira"))
    .pivot('mes', ['January', 'February'])
    .agg(count('*'))
    # .orderBy(col('cartao_bandeira'), col("mes"))
)

#para fazer consultas SQL direto preciso passar ''' no inicio e ''' no final

spark.sql('''
      SELECT *
      FROM {tabela}
      WHERE estado IN {lista_estados}
''', tabela = df, lista_estados = ('RR', 'GO'))

tabela_sp = df.where(col('estado') == 'SP')

tabela_rj = df.where(col('estado') == 'RJ')

#também tem a função registerTempTable, pra criar uma tabela temporária no código para algum uso, caso necessário

df = spark.read.parquet('/content/LOGINS.parquet')

# posso passar um partitiionBy no write/extração do df, assim ele ja salva separado pelos dados das colunas argumento do partition
df.write.save('output', mode='overwrite', format='csv', partitionBy=['estado', 'cor_favorita'])

#outro modo de salvar

df.write.saveAsTable('db.nome_tabela', mode='overwrite')

# como fazemos no trabalho
df.write.mode('overwrite').save('database_usada.nome_tabela')

#criando um DF, bom saber para testes de saída/resultado de funções

#a partir de um dicionario

dados = [
    {'nome':'Bruno', 'idade': 29},
    {'nome': 'Aline', 'idade': 29},
    {'nome': 'Ana', 'idade': 54}

]

dados

# agora criando um df a partir do dicionario
spark.createDataFrame(dados)

#criar a partir de uma tupla - o que é melhor
dados2 = [
    ('Bruno', 30),
    ('Aline', 29),
    ('Ana', 54)
]

dados2

spark.createDataFrame(dados2, ['Nome', 'Idade'])

# Schema com o TIPO de dado la do import pyspark.sql.types import * ou as T
#passo o tipo que será o dado da coluna. Se True - permite valores nules / Se False = NÃO permite NULOS

schema = StructType([
    StructField('Nome', StringType(), True),
    StructField('Idade', IntegerType(), True)
])

schema

#com o schema criado, utilizo ele como argumento na hora de criar o dataframe
df1 = spark.createDataFrame(dados2, schema)

df1.printSchema()

#também dá pra criar como Row
from pyspark.sql import Row

rdd = [
    Row('Bruno', 29),
    Row('Aline', 29)
]

rdd

spark.createDataFrame(rdd, "_1: string, _2: int")

# #UDF User Defined Functions - quero CRIAR as minhas funções
# from pyspark.sql.functions import udf -- no caso nao precisa, pq ja importei TODAS sql.functions no início, aqui seri caso queira trazer APENAS o UDF
# criei a função, deve passar o @udf antes da função. deve ter importado o sql.types e após, basta chamar quando quiser.

@udf(returnType = 'ARRAY<STRING>')
def nome_estado(sigla):
  if sigla == 'SP':
    return ['São Paulo', 'Sudeste']
  elif sigla == 'RJ':
    return ['Rio de Janeiro', 'Sudeste']
  elif sigla == 'MG':
    return ['Minas Gerais', 'Sudeste']
  else:
    return ['Outros', 'Demais']

#     @udf(returnType = StringType())
# def nome_estado(sigla):
#   if sigla == 'SP':
#     return 'São Paulo'
#   elif sigla == 'RJ':
#     return 'Rio de Janeiro'
#   else:
#     return 'Outros'

df.select(col('estado'), element_at(nome_estado(col('estado')), 1))

#funções janeladas
autores = spark.read.parquet('/content/AUTORES.parquet').alias('autores')
livros = spark.read.parquet('/content/LIVROS.parquet').select('id', 'data_lancamento', 'preco').alias('livros')
compras = spark.read.parquet('/content/COMPRAS.parquet').select('id', 'data', 'cd_livro', 'cd_cliente').alias('compras')

df = compras.join(livros, compras.cd_livro ==livros.id).join(autores, livros.id == autores.id).drop('livros.id', 'autores.id')
df

#pra trazer as funções janeladas
from pyspark.sql.window import *

#passei o parâmetro que quero utilizar para janelar
#no window2 quero que ele me ordene numerando a compra de determinado cliente, então vou particionar para que ela reinicie toda vez que mudar de cliente. Ainda, as numerações serão na ordem que as compras foram feitas
#window3 estou particionando por autor e ordenando pelo lancamento de seus livros
window1 = Window.orderBy('compras.id')
window2 = Window.partitionBy('cd_cliente').orderBy('data')
window3 = Window.partitionBy('autor').orderBy('data_lancamento')

#a primeira será a numeração de linhas. utilizo o over pq quero que ele passe por todo o DF e não uma coluna em específico
# veja que no ordem_compra, ele reinicia a contagem toda vez que mudar o cd_cliente, e passei a ordenação pela data, pois assim ele sabe por onde começar a contagem.
#atenção com o sum e o over. o over deve estar FORA do sum().over. Não vá se perder nos parênteses. com essa ultima, na coluna total_acumulado_cliente, veja que o acumulado está atualizado para o momento DAQUELA compra.
#então no cliente 1010444, sei que na compra 2 ele ja tinha gastado 258,53. na 3 403, na 4 612. Mostra a evolução do acumulado de compra em compra. Ainda, como coloquei a col preco na visualização, sei o valor individual de cada livro que comprou.
(
    df
    .withColumn('num_linha', row_number().over(window1))
    .withColumn('ordem_compra',  row_number().over(window2))
    # .dropDuplicates(['cd_livro', 'autor'])
    # .withColumn('ordem_lancamento', row_number().over(window3))
    .withColumn('total_acumulado_cliente', round(sum(col('preco')).over(window2), 2))
)

